# ğŸ—‚ï¸ Fase 1: MinIO (Data Lake)

**Objetivo**: Configurar y validar MinIO como almacÃ©n de datos tipo S3, y cargar un modelo de datos profesional

## ğŸ¯ QuÃ© aprenderÃ¡s en esta fase

- **Almacenamiento tipo S3**: Conceptos de buckets y objetos
- **MinIO**: ConfiguraciÃ³n y uso bÃ¡sico
- **API S3**: Interfaz estÃ¡ndar para almacenamiento
- **Docker networking**: Redes aisladas para servicios
- **Health checks**: ValidaciÃ³n automÃ¡tica de servicios
- **OrganizaciÃ³n de Data Lake**: Estructura profesional de datos
- **Modelo estrella**: Dimensiones y tablas de hechos

## ğŸ—ï¸ Arquitectura de esta fase

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MinIO         â”‚
â”‚ (Data Lake S3)  â”‚
â”‚                 â”‚
â”‚ Port 9000: API  â”‚
â”‚ Port 9001: UI   â”‚
â”‚                 â”‚
â”‚ Bucket:         â”‚
â”‚ â””â”€ datalake/    â”‚
â”‚   â””â”€modelo-ventas/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš¡ Inicio rÃ¡pido

```bash
# 1. Configurar variables
cp .env.example .env

# 2. Levantar MinIO
make up
# O manualmente: docker compose up -d

# 3. Validar que funciona
make test
# O manualmente: ./test.sh

# 4. Acceder a la consola web
# http://localhost:9001
# Usuario: admin / ContraseÃ±a: admin12345
```

## ğŸ”§ ConfiguraciÃ³n detallada

### Variables de entorno (.env)

| Variable | DescripciÃ³n | Valor por defecto |
|----------|-------------|-------------------|
| `MINIO_API_PORT` | Puerto para API S3 | 9000 |
| `MINIO_CONSOLE_PORT` | Puerto para consola web | 9001 |
| `MINIO_ROOT_USER` | Usuario administrador | admin |
| `MINIO_ROOT_PASSWORD` | ContraseÃ±a admin | admin12345 |
| `MINIO_DEFAULT_BUCKET` | Bucket por defecto | datalake |

### Puertos utilizados

- **9000**: API S3 compatible (para aplicaciones)
- **9001**: Consola web MinIO (para humanos)

## ğŸ“Š Modelo de Datos: Ventas (Esquema Estrella)

Este proyecto incluye un **modelo de datos profesional** con:

### ğŸ“‹ Dimensiones (7 tablas)
- **dim_date.csv**: Calendario 2023-2024 (731 filas) con festivos espaÃ±oles
- **dim_currency.csv**: 4 monedas (EUR, USD, GBP, MXN)  
- **dim_channel.csv**: 5 canales de venta
- **dim_store.csv**: 30 tiendas (ES, FR, PT, MX, UK)
- **dim_product.csv**: 300 productos en 6 categorÃ­as
- **dim_salesperson.csv**: 80 vendedores con equipos
- **dim_customer.csv**: 5,000 clientes B2C/B2B

### ğŸ“ˆ Hechos (2 tablas)
- **fact_exchangerates.csv**: Tipos de cambio diarios
- **fact_sales.csv**: **200,000 lÃ­neas de ventas** con mÃ©tricas completas

## ğŸ“ Estructura del Data Lake

```
datalake/
â””â”€â”€ modelo-ventas/
    â””â”€â”€ raw/                     # Datos originales
        â”œâ”€â”€ dimensions/          # Tablas de dimensiones
        â”‚   â”œâ”€â”€ dim_date.csv
        â”‚   â”œâ”€â”€ dim_currency.csv
        â”‚   â”œâ”€â”€ dim_channel.csv
        â”‚   â”œâ”€â”€ dim_store.csv
        â”‚   â”œâ”€â”€ dim_product.csv
        â”‚   â”œâ”€â”€ dim_salesperson.csv
        â”‚   â””â”€â”€ dim_customer.csv
        â”œâ”€â”€ facts/               # Tablas de hechos  
        â”‚   â”œâ”€â”€ fact_exchangerates.csv
        â”‚   â””â”€â”€ fact_sales.csv
        â””â”€â”€ metadata/            # DocumentaciÃ³n
            â””â”€â”€ README_modelo_ventas.md
```

## ğŸš€ CÃ³mo cargar el modelo de datos

### Paso 1: Crear estructura
1. Acceder a http://localhost:9001 (admin/admin12345)
2. Crear bucket `datalake`
3. Crear carpetas: `modelo-ventas/raw/dimensions/`, `facts/`, `metadata/`

### Paso 2: Subir dimensiones
Navegar a `datalake/modelo-ventas/raw/dimensions/` y subir:
- dim_date.csv
- dim_currency.csv  
- dim_channel.csv
- dim_store.csv
- dim_product.csv
- dim_salesperson.csv
- dim_customer.csv

### Paso 3: Subir hechos
Navegar a `datalake/modelo-ventas/raw/facts/` y subir:
- fact_exchangerates.csv
- fact_sales.csv

### Paso 4: Subir metadata
Navegar a `datalake/modelo-ventas/raw/metadata/` y subir:
- README_modelo_ventas.md

## ğŸ§ª Validaciones y tests

### Test automÃ¡tico bÃ¡sico
```bash
make test
```

### ValidaciÃ³n del modelo de datos
```bash
# Listar todo el modelo cargado
docker exec fabric-minio mc ls -r local/datalake/modelo-ventas/

# Verificar dimensiones
docker exec fabric-minio mc ls local/datalake/modelo-ventas/raw/dimensions/

# Verificar hechos  
docker exec fabric-minio mc ls local/datalake/modelo-ventas/raw/facts/

# Ver estadÃ­sticas de archivos
docker exec fabric-minio mc stat local/datalake/modelo-ventas/raw/facts/fact_sales.csv
```

### ExploraciÃ³n rÃ¡pida
```bash
# Ver las primeras lÃ­neas de ventas
docker exec fabric-minio mc cat local/datalake/modelo-ventas/raw/facts/fact_sales.csv | head -5

# Contar registros de fact_sales
docker exec fabric-minio mc cat local/datalake/modelo-ventas/raw/facts/fact_sales.csv | wc -l
```

## ğŸ› ï¸ Comandos disponibles

```bash
# GestiÃ³n bÃ¡sica
make up          # Levantar MinIO
make down        # Parar MinIO
make restart     # Reiniciar MinIO
make logs        # Ver logs en tiempo real

# Validaciones
make test        # Test completo automatizado
make health      # Solo health check
make create-bucket  # Crear bucket por defecto

# Utilidades
make clean       # Limpiar datos (Â¡cuidado!)
make backup      # Backup de datos
make restore     # Restaurar backup
```

## ğŸš¨ Troubleshooting

### Puerto 9000/9001 ocupado
```bash
# Verificar quÃ© usa el puerto
netstat -ano | findstr :9000

# Cambiar puerto en .env
MINIO_API_PORT=9002
MINIO_CONSOLE_PORT=9003

# Reiniciar
make restart
```

### No se puede acceder a la consola
```bash
# Verificar que el contenedor estÃ© corriendo
docker ps | grep minio

# Ver logs de MinIO
make logs

# Verificar health check
docker inspect fabric-minio | grep Health -A 10
```

### Error de permisos en carpeta data/
```bash
# En Linux/macOS
sudo chown -R $USER:$USER ../../data/minio

# En Windows con WSL2
# AsegÃºrate de estar en una carpeta WSL, no en /mnt/c/
```

## ğŸ“š Conceptos importantes

### Â¿QuÃ© es MinIO?
MinIO es un servidor de almacenamiento de objetos compatible con Amazon S3. Es perfecto para:
- **Data Lakes**: Almacenar grandes volÃºmenes de datos sin estructura
- **Backups**: Respaldos distribuidos y resilientes  
- **Data pipelines**: Punto central para ETL/ELT
- **ML/AI**: Almacenar datasets y modelos

### Â¿Por quÃ© empezar con MinIO?
1. **Sin dependencias**: Funciona solo, sin necesidad de otros servicios
2. **Interfaz familiar**: API S3 es estÃ¡ndar de facto
3. **FundaciÃ³n sÃ³lida**: Todos los demÃ¡s servicios se conectarÃ¡n aquÃ­
4. **FÃ¡cil de debuggear**: Interfaz web clara y logs comprensibles

### Modelo Estrella en Data Lake
- **Datos raw**: Se almacenan tal como vienen de origen
- **OrganizaciÃ³n**: Por tipo (dimensiones/hechos) para facilitar ETL
- **Flexibilidad**: Permite mÃºltiples usos (BI, ML, anÃ¡lisis)
- **Escalabilidad**: Estructura profesional preparada para crecer

## âœ… Criterios de Ã©xito

Antes de pasar a la Fase 2, asegÃºrate de que:
- [ ] MinIO arranca sin errores
- [ ] Puedes acceder a http://localhost:9001
- [ ] Bucket `datalake` creado con estructura modelo-ventas
- [ ] 9 archivos CSV subidos y organizados
- [ ] Puedes listar y explorar archivos con comandos mc
- [ ] El modelo de datos estÃ¡ completo (200k+ registros en fact_sales)
- [ ] Entiendes la estructura de Data Lake profesional

## ğŸ¯ PrÃ³xima fase

Una vez que MinIO y tu modelo de datos funcionen perfectamente, continÃºa con:
**[Fase 2: PostgreSQL](../fase-2-postgres/README.md)**

AhÃ­ importaremos tu modelo estrella a PostgreSQL y verÃ¡s las dimensiones y hechos en acciÃ³n como base de datos relacional.

---

ğŸ’¡ **Tip**: Este modelo de 200,000 registros de ventas es perfecto para las siguientes fases. Representa un escenario realista que encontrarÃ¡s en proyectos profesionales.